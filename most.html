<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1200px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>MOST: Multiple Object localization with Self-supervised Transformers for object discovery.</title>
	<meta property="og:image" content="./resources/teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="MOST: Multiple Object localization with Self-supervised Transformers for object discovery." />
	<meta property="og:description" content="Multiple object localization using SSL transformers." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:30px">MOST: Multiple Object localization with Self-supervised Transformers for object discovery.</span>
		<br>
		<br>
		<span style="font-size:24">ICCV 2023 <font color="red"><strong>(Oral)</strong></font></span>
		<br>
		<br>
		<table align=center width=800px>
			<table align=center width=1000px>
				<tr>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://rssaketh.github.io/">Saketh Rambhatla</a></span>
						</center>
					</td>
					
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://imisra.github.io">Ishan Misra</a></span>
						</center>
					</td>
				<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://engineering.jhu.edu/ece/faculty/rama-chellappa/">Rama Chellappa</a></span>
						</center>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a></span>
						</center>
					</td>
				</tr>
			</table>
			<br>
			<table align=center width=700px>
				<tr>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/abs/2304.05387'>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href='data/most_supplementary.pdf'>[Supplementary]</a></span>
						</center>
					</td>

					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="./data/most_poster.pdf">[Poster]</a></span><br>
						</center>
					</td>

					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://github.com/rssaketh/MOST/">[GitHub]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
			<br>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:700px" src="../images/most/teaser_nohuman.png"/>
					</center>
				</td>
			</tr>
		</table>
		<br>
		<table align=center width=1000px>
			<tr>
				<td>
<strong>Top</strong>: Methods like <a href="https://arxiv.org/abs/2109.14279">LOST</a> (shown in figure), <a href="https://arxiv.org/abs/2202.11539">TokenCut</a> identify and localize the most salient foreground object and hence can detect only one object per image.
<br>
<strong>Bottom</strong>: MOST is a simple, yet effective method that localizes multiple objects per image without training.				</td>
			</tr>
		</table>
		<br>
	</center>

	<hr>

	<table align=center width=1000px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				We tackle the challenging task of unsupervised object localization in this work. Recently, transformers trained with self-supervised learning have been shown to exhibit object localization properties without being trained for this task. In this work, we present Multiple Object localization with Self-supervised Transformers (MOST) that uses features of transformers trained using self-supervised learning to localize multiple objects in real world images. MOST analyzes the similarity maps of the features using box counting; a fractal analysis tool to identify tokens lying on foreground patches. The identified tokens are then clustered together, and tokens of each cluster are used to generate bounding boxes on foreground regions. Unlike recent state-of-the-art object localization methods, MOST can localize multiple objects per image and outperforms SOTA algorithms on several object localization and discovery benchmarks on PASCAL-VOC 07, 12 and COCO20k datasets. Additionally, we show that MOST can be used for self-supervised pre-training of object detectors, and yields consistent improvements on fully, semi-supervised object detection and unsupervised region proposal generation.			</td>
		</tr>
	</table>
	<br>
	<hr>

	<table align=center width=1000px>
		<center><h1>Motivation</h1></center>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:1000px" src="../images/most/motiv.png" /></td>
				</center>
			</td>
		</tr>
		</table>
		<br>
	<table align=center width=1000px>
		<tr>
			<td> Example showing similarity maps of tokens within background and foreground for an image from the COCO dataset. In the figure above, we show three examples of the similarity maps of a token (shown in red) picked on the background (column 2) and foreground (columns 3, 4). Tokens within foreground patches have higher correlation than the ones on background. This results in the similarity maps of foreground patches being less "spatially" random than the ones on the background. The task then becomes to analyze the similarity maps and identify the ones with less spatial randomness.
							</td>
		</tr>
	</table>
	<br>
	<!-- <hr>
	<center><h1>Talk</h1></center>
	<p align="center">
		<iframe width="660" height="395" src="https://www.youtube.com/embed/dQw4w9WgXcQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
	</p>

	<table align=center width=800px>
		<br>
		<tr>
			<center>
				<span style="font-size:28px"><a href=''>[Slides]</a>
				</span>
			</center>
		</tr>
	</table> -->
	<hr>

	<center><h1>Approach Overview</h1></center>

	<!-- <table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table> -->
	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:1000px" src="../images/most/most_pipeline.png" /></td>
				</center>
			</td>
		</tr>
	</table>
	<br>
	<table align=center width=1000px>
		<center>
			<tr>
				<td>
					<p class="text-justify">
MOST operates on features extracted from transformers trained using <a href="https://arxiv.org/abs/2104.14294">DINO</a>. The features are used to compute the outer product A. Each row of A is analyzed by the entropy-based box analysis (EBA) module that identifies tokens extracted from foreground patches. These patches are clustered using spatial locations as features to form pools. Each pool is then post-processed to generate a bounding box.					</p>

				</td>
			</tr>
		</center>
	</table>
	<!-- <table align=center width=800px>
		<br>
		<tr><center>
			<span style="font-size:28px">&nbsp;<a href='https://github.com/richzhang/webpage-template'>[GitHub]</a>
			</center>
		</span>
	</table> -->
	<br>
	<hr>

	<center><h1>Qualitative Results</h1></center>
	<!-- <table align=center width=850px> -->
		<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:1000px" src="../images/most/q_res_coco20k_nohuman.png" /></td>
				</center>
			</td>
		</tr>
		</table>
		<br>

		<!--<table align=center width=1000px>
		<center>
			<tr>
				<td>
					<p class="text-justify">
						Qualitative results of MOST on COCO20k dataset: MOST can localize multiple objects per image without training. Localization ability of MOST is not limited by the biases of annotators and can localize rocks, branches, water bodies etc. 
					</p>
				</td>
			</tr>
		</center>
		</table>
		<br>-->
<hr style="width:50%">
<br>
<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:1000px" src="../images/most/q_res_voc07_12_nohuman.png" /></td>
				</center>
			</td>
		</tr>
		</table>
		<br>

		<table align=center width=1000px>
		<center>
			<tr>
				<td>
					<p class="text-justify">
						Qualitative results of MOST on COCO20k (<strong>Top</strong>) and PASCAL VOC 2007+12 (<strong> (Bottom) </strong>) datasets: MOST can localize multiple objects per image without training. Localization ability of MOST is not limited by the biases of annotators and can localize rocks, branches, water bodies etc. 
					</p>
				</td>
			</tr>
		</center>
		</table>
		<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:1000px" src="../images/most/most_dut_omron_sd_nohuman.png" /></td>
				</center>
			</td>
		</tr>
		</table>
		<br>
		<hr style="width:50%">
		<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:1000px" src="../images/most/most_duts_sd_nohuman.png" /></td>
				</center>
			</td>
		</tr>
		</table>
		<br>
		<hr style="width:50%">
		<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:1000px" src="../images/most/most_ecssd_sd_nohuman.png" /></td>
				</center>
			</td>
		</tr>
		</table>
		<br>


		<table align=center width=1000px>
		<center>
			<tr>
				<td>
					<p class="text-justify">
MOST can easily be extended for the task of unsupervised saliency detection. We choose the object identified by the largest cluster as the salient object and demonstrate results on DUT-OMRON (<strong>Top</strong>), DUTS (<strong>Middle</strong>), ECSSD (<strong>Bottom</strong>) datasets. Each row shows two examples of input and the output of MOST. In each example, the first image is the input, the second image is the mask generated using the largest cluster, i.e. the output. The third image is the output mask when all the clusters are used and the fourth image is the ground truth. When only one salient object exists in the input (row-1) using all the clusters results in segmenting non salient objects. In the presence of multiple instances of the salient object (row-2), picking the largest cluster results in segmenting only a single instance. Finally, in row-3, we show some failure cases of MOST. Since all the three datasets consists of a majority of images with a single instance, we choose the the mask generated from the largest cluster as our output.
					</p>
				</td>
			</tr>
		</center>
		</table>
		<br>


	<hr>
	<table align=center width=750px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><img class="layered-paper-big" style="height:175px" src="../images/most_paper.png"/></td>
			<td><span style="font-size:14pt">S. Rambhatla, I. Misra, R. Chellappa, A. Shrivastava.<br>
				<b>MOST: Multiple Object localization with Self-supervised Transformers for object discovery.</b><br>
				ICCV, 2023.<br> 
				<!-- (hosted on <a href="">ArXiv</a>)<br> -->
				(<a href="https://arxiv.org/abs/2304.05387">Paper</a> | 
				<a href="./data/most_supplementary.pdf">Supplementary</a>)<br>
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>
	<br>

	<hr>
	<table align=center width=900px>
		<tr>
				<p style="text-align:right;font-size:small;">
               <a href="https://github.com/richzhang/webpage-template">Template credits</a>
              </p>
		</tr>
	</table>

<br>
</body>
</html>



